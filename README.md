# ğŸš Desafio de Data Engineer - Civitas

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o do desafio tÃ©cnico para a vaga de **Engenheiro(a) de Dados** na [Civitas](https://civitas.rio/).  
O objetivo Ã© construir uma **pipeline ELT em arquitetura Medallion (Bronze â†’ Silver â†’ Gold)**,  
utilizando **Prefect 1.4.1** e **Google Cloud (GCS + BigQuery)** para ingestÃ£o, armazenamento e transformaÃ§Ã£o de dados do BRT em tempo real.

<a name="docs"/>

### DocumentaÃ§Ã£o e Testes

## ğŸ“‹ Ãndice

- [Ambiente e Ferramentas](#tools)
- [Arquitetura](arquitecture)
  - [Arquitetura das Pipelines](#pipelines)
  - [Arquitetura das Queries](#dbt)
  - [Estrutura de DiretÃ³rios](#directories)
  - [Camadas de modelagem](#model)
  - [DocumentaÃ§Ã£o e Testes](#docs)
- [Setup do Ambiente de Desenvolvimento Local](#setup)
  - [1. PrÃ©-requisitos](#setup1)
  - [2. CriaÃ§Ã£o do ambiente virtual](#setup2)
  - [3. Instalar dependÃªncias](#setup3)
  - [4. ConfiguraÃ§Ã£o do Docker](#setup4)
  - [5. Subir o Prefect Server Local e Iniciar o server](#setup5)
  - [6. VariÃ¡veis de ambiente e credenciais GCP](#setup6)

---
<a name="tools"/>

###  âš™ï¸ Ambiente e Ferramentas

| Componente | VersÃ£o | FunÃ§Ã£o |
|-------------|---------|--------|
| **Python** | 3.10 | Linguagem principal |
| **Prefect** | 1.4.1 | OrquestraÃ§Ã£o da pipeline |
| **Docker / Docker Compose** | 4.4.4 / 1.29.2 | Infraestrutura local e agente |
| **Google Cloud SDK** | latest | Armazenamento e consultas |
| **DBT (Data Build Tool)** | latest | TransformaÃ§Ã£o de dados no BigQuery |
---
<a name="arquitecture"/>

## Arquitetura

<a name="pipelines"/>

### Arquitetura das Pipelines
Os pipelines seguem o padrÃ£o ELT orquestrado pelo Prefect.

<a name="dbt"/>

### Arquitetura das Queries (DBT)
As transformaÃ§Ãµes de dados utilizam **DBT (Data Build Tool)**.

<a name="directories"/>

### Estrutura de DiretÃ³rios
dbt_project/
â”œâ”€â”€ dbt_project.yml           # ConfiguraÃ§Ã£o do projeto DBT
â”œâ”€â”€ packages.yml              # Pacotes DBT externos (dbt_utils, dbt_expectations, elementary)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bronze/               # Camada Bronze - dados brutos (tabela externa BigQuery)
â”‚   â”‚   â””â”€â”€ external_brt_data.sql
â”‚   â”œâ”€â”€ silver/               # Camada Silver - limpeza e normalizaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ consolidated_brt_data.sql
â”‚   â”œâ”€â”€ gold/                 # Camada Gold - agregaÃ§Ãµes e mÃ©tricas
â”‚   â”‚   â””â”€â”€ vehicles_dashboard.sql
â”‚   â””â”€â”€ schema.yml            # DocumentaÃ§Ã£o e testes de qualidade
â””â”€â”€ tests/                    # Testes customizados adicionais

<a name="model"/>

### Camadas de modelagem

- Bronze: Dados brutos ingeridos do GCS (Google Cloud Storage) e disponibilizados no BigQuery como tabela externa.	`models/bronze/external_brt_data.sql`
- Silver:	Dados limpos, normalizados e enriquecidos.	`models/silver/consolidated_brt_data.sql`
- Gold:	MÃ©tricas e agregaÃ§Ãµes prontas para consumo.	`models/gold/vehicles_dashboard.sql`

<a name="docs"/>

### DocumentaÃ§Ã£o e Testes

Os arquivos .yml contÃªm documentaÃ§Ã£o detalhada de cada modelo e testes de qualidade com dbt_expectations, garantindo integridade e consistÃªncia dos dados.
As descriÃ§Ãµes sÃ£o propagadas automaticamente para o BigQuery via +persist_docs.

Exemplo (trecho de models/schema.yml):
```
models:
  - name: consolidated_brt_data
    description: "Camada Silver com dados normalizados e enriquecidos do BRT."
    tests:
      - dbt_expectations.expect_table_row_count_to_be_between:
          arguments:
            min_value: 1
      - dbt_expectations.expect_column_values_to_not_be_null:
          arguments:
            column_name: codigo
```
---
<a name="setup"/>

## âš™ï¸ Setup do Ambiente de Desenvolvimento Local

<a name="setup1"/>

### 1. PrÃ©-requisitos

- Instalar e ativar **Docker Desktop**
- Instalar **Python 3.10**
- Autenticar o **Google Cloud SDK** (`gcloud auth login`)
- Habilitar as APIs:
  - **Cloud Storage API**
  - **BigQuery API**

<a name="setup2"/>

### 2. CriaÃ§Ã£o do ambiente virtual

```bash
python3.10 -m venv venv
source venv/bin/activate
```
<a name="setup3"/>

### 3. Instalar dependÃªncias

As versÃµes foram fixadas para garantir compatibilidade com Prefect 1.4.1 e macOS ARM.
Instale as dependÃªncias a partir do arquivo requirements.txt:

```
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt --no-deps
```
<a name="setup4"/>

### 4. ConfiguraÃ§Ã£o do Docker 

Linux:
```bash
export DOCKER_HOST=unix:///var/run/docker.sock
sudo systemctl start docker
```

MacOS (Apple Silicon):
```
export DOCKER_HOST=unix://$HOME/.docker/run/docker.sock
```

Verifique se o Docker estÃ¡ acessÃ­vel:

```
docker ps
```

Se o comando listar containers, o daemon do Docker estÃ¡ ativo e acessÃ­vel.

Dica: adicione o comando de export DOCKER_HOST ao seu ~/.zshrc (macOS) ou ~/.bashrc (Linux)
para que a configuraÃ§Ã£o seja carregada automaticamente em novos terminais.

<a name="setup5"/>

### 5. Subir o Prefect Server Local e Iniciar o server

Execute:
```
prefect backend server
```
```
prefect server start
```

VocÃª deve ver a mensagem:
```
Visit http://localhost:8080 to get started, or check out the docs at https://docs.prefect.io
```
<a name="setup6"/>

### 6. VariÃ¡veis de ambiente e credenciais GCP

Para rodar localmente, copie o arquivo .env.example para .env e atualize os valores conforme sua configuraÃ§Ã£o:
```
GCP_PROJECT=desafio-civitas-brt-ari
GCS_BUCKET=civitas-brt-data-ari
GOOGLE_APPLICATION_CREDENTIALS=/Users/<usuario>/desafio-civitas-brt-ari/prefect-gcp-access-xxxx.json
```

Caso nÃ£o possua acesso ao GCP, Ã© possÃ­vel testar o pipeline apenas atÃ© a geraÃ§Ã£o dos arquivos CSV locais.

#### ConfiguraÃ§Ã£o de credenciais GCP para o DBT e Prefect

Tanto o Prefect quanto o DBT utilizam a mesma conta de serviÃ§o (Service Account)
para acessar o Google Cloud Storage (GCS) e o BigQuery.

No arquivo profiles.yml do DBT, o caminho da credencial Ã© definido dinamicamente usando a variÃ¡vel de ambiente:
```
keyfile: "{{ env_var('GOOGLE_APPLICATION_CREDENTIALS') }}"
```
Isso garante seguranÃ§a e portabilidade, evitando caminhos hardcoded.

Antes de rodar o DBT, carregue o .env:
```
source venv310/bin/activate
export $(grep -v '^#' .env | xargs)
```

Em seguida, execute:
```
cd dbt_project
dbt run
dbt test
```
â„¹ï¸ Todas as variÃ¡veis de ambiente (GCP, Prefect e DBT) sÃ£o centralizadas no arquivo .env para garantir seguranÃ§a e facilidade de configuraÃ§Ã£o.



