# üöç Desafio de Data Engineer - Civitas

Este reposit√≥rio cont√©m a implementa√ß√£o do desafio t√©cnico para a vaga de **Engenheiro(a) de Dados** na [Civitas](https://civitas.rio/).  
O objetivo √© construir uma **pipeline ELT em arquitetura Medallion (Bronze ‚Üí Silver ‚Üí Gold)**,  
utilizando **Prefect 1.4.1** e **Google Cloud (GCS + BigQuery)** para ingest√£o, armazenamento e transforma√ß√£o de dados do BRT em tempo real.

<a name="docs"/>

### Documenta√ß√£o e Testes

## üìã √çndice

- [Ambiente e Ferramentas](#tools)
- [Arquitetura](arquitecture)
  - [Arquitetura das Pipelines](#pipelines)
  - [Arquitetura das Queries](#dbt)
  - [Estrutura de Diret√≥rios](#directories)
  - [Camadas de modelagem](#model)
  - [Documenta√ß√£o e Testes](#docs)
- [Instru√ß√µes para Execu√ß√£o Local](#setup)
  - [1. Pr√©-requisitos](#setup1)
  - [2. Cria√ß√£o do ambiente virtual](#setup2)
  - [3. Instalar depend√™ncias](#setup3)
  - [4. Configura√ß√£o do Docker](#setup4)
  - [5. Subir o Prefect Server Local e Iniciar o server](#setup5)
  - [6. Vari√°veis de ambiente e credenciais GCP](#setup6)
  - [7. Rodar o flow principal](#setup7)
  - [8. Configura√ß√£o de credenciais GCP para o DBT e Prefect](#setup8)
  - [9. Executar o DBT](#setup9)

---
<a name="tools"/>

###  ‚öôÔ∏è Ambiente e Ferramentas

| Componente | Vers√£o | Fun√ß√£o |
|-------------|---------|--------|
| **Python** | 3.10 | Linguagem principal |
| **Prefect** | 1.4.1 | Orquestra√ß√£o da pipeline |
| **Docker / Docker Compose** | 4.4.4 / 1.29.2 | Infraestrutura local e agente |
| **Google Cloud SDK** | latest | Armazenamento e consultas |
| **DBT (Data Build Tool)** | latest | Transforma√ß√£o de dados no BigQuery |
---
<a name="arquitecture"/>

## Arquitetura

<a name="pipelines"/>

### Arquitetura das Pipelines
Os pipelines seguem o padr√£o ELT orquestrado pelo Prefect.

<a name="dbt"/>

### Arquitetura das Queries (DBT)
As transforma√ß√µes de dados utilizam **DBT (Data Build Tool)**.

<a name="directories"/>

### Estrutura de Diret√≥rios
dbt_project/
‚îú‚îÄ‚îÄ dbt_project.yml           # Configura√ß√£o do projeto DBT
‚îú‚îÄ‚îÄ packages.yml              # Pacotes DBT externos (dbt_utils, dbt_expectations, elementary)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/               # Camada Bronze - dados brutos (tabela externa BigQuery)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ external_brt_data.sql
‚îÇ   ‚îú‚îÄ‚îÄ silver/               # Camada Silver - limpeza e normaliza√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ consolidated_brt_data.sql
‚îÇ   ‚îú‚îÄ‚îÄ gold/                 # Camada Gold - agrega√ß√µes e m√©tricas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vehicles_dashboard.sql
‚îÇ   ‚îî‚îÄ‚îÄ schema.yml            # Documenta√ß√£o e testes de qualidade
‚îî‚îÄ‚îÄ tests/                    # Testes customizados adicionais

<a name="model"/>

### Camadas de modelagem

- Bronze: Dados brutos ingeridos do GCS (Google Cloud Storage) e disponibilizados no BigQuery como tabela externa.	`models/bronze/external_brt_data.sql`
- Silver:	Dados limpos, normalizados e enriquecidos.	`models/silver/consolidated_brt_data.sql`
- Gold:	M√©tricas e agrega√ß√µes prontas para consumo.	`models/gold/vehicles_dashboard.sql`

<a name="docs"/>

### Documenta√ß√£o e Testes

Os arquivos .yml cont√™m documenta√ß√£o detalhada de cada modelo e testes de qualidade com dbt_expectations, garantindo integridade e consist√™ncia dos dados.
As descri√ß√µes s√£o propagadas automaticamente para o BigQuery via +persist_docs.

Exemplo (trecho de models/schema.yml):
```
models:
  - name: consolidated_brt_data
    description: "Camada Silver com dados normalizados e enriquecidos do BRT."
    tests:
      - dbt_expectations.expect_table_row_count_to_be_between:
          arguments:
            min_value: 1
      - dbt_expectations.expect_column_values_to_not_be_null:
          arguments:
            column_name: codigo
```
---
<a name="setup"/>

## ‚öôÔ∏è Instru√ß√µes para Execu√ß√£o Local]

<a name="setup1"/>

### 1. Pr√©-requisitos

- Instalar e ativar **Docker Desktop**
- Instalar **Python 3.10**
- Autenticar o **Google Cloud SDK** (`gcloud auth login`)
- Habilitar as APIs:
  - **Cloud Storage API**
  - **BigQuery API**

<a name="setup2"/>

### 2. Cria√ß√£o do ambiente virtual

```bash
python3.10 -m venv venv
source venv/bin/activate
```
<a name="setup3"/>

### 3. Instalar depend√™ncias

As vers√µes foram fixadas para garantir compatibilidade com Prefect 1.4.1 e macOS ARM.
Instale as depend√™ncias a partir do arquivo requirements.txt:

```
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt --no-deps
```
<a name="setup4"/>

### 4. Configura√ß√£o do Docker 

Linux:
```bash
export DOCKER_HOST=unix:///var/run/docker.sock
sudo systemctl start docker
```

MacOS (Apple Silicon):
```
export DOCKER_HOST=unix://$HOME/.docker/run/docker.sock
```

Verifique se o Docker est√° acess√≠vel:

```
docker ps
```

Se o comando listar containers, o daemon do Docker est√° ativo e acess√≠vel.

Dica: adicione o comando de export DOCKER_HOST ao seu ~/.zshrc (macOS) ou ~/.bashrc (Linux)
para que a configura√ß√£o seja carregada automaticamente em novos terminais.

<a name="setup5"/>

### 5. Subir o Prefect Server Local e Iniciar o server

Execute:
```
prefect backend server
```
```
prefect server start
```

Voc√™ deve ver a mensagem:
```
Visit http://localhost:8080 to get started, or check out the docs at https://docs.prefect.io
```
<a name="setup6"/>

### 6. Vari√°veis de ambiente e credenciais GCP

Para rodar localmente, copie o arquivo .env.example para .env e atualize os valores conforme sua configura√ß√£o:
```
GCP_PROJECT=desafio-civitas-brt-ari
GCS_BUCKET=civitas-brt-data-ari
GOOGLE_APPLICATION_CREDENTIALS=/Users/<usuario>/desafio-civitas-brt-ari/prefect-gcp-access-xxxx.json
```

Caso n√£o possua acesso ao GCP, √© poss√≠vel testar o pipeline apenas at√© a gera√ß√£o dos arquivos CSV locais.

<a name="setup7"/>

### 7. Rodar o flow principal

Para rodar o fluxo principal, assumindo que voc√™ ainda est√° no ambiente virtual `venv/bin/activate` iniciado [acima](#setup2), execute:
```
python src/schedule.py
```
<a name="setup8"/>
### 8. Configura√ß√£o de credenciais GCP para o DBT e Prefect

Tanto o Prefect quanto o DBT utilizam a mesma conta de servi√ßo (Service Account)
para acessar o Google Cloud Storage (GCS) e o BigQuery.

No arquivo profiles.yml do DBT, o caminho da credencial √© definido dinamicamente usando a vari√°vel de ambiente:
```
keyfile: "{{ env_var('GOOGLE_APPLICATION_CREDENTIALS') }}"
```
Isso garante seguran√ßa e portabilidade, evitando caminhos hardcoded.

Antes de rodar o DBT, carregue o .env:
```
source venv/bin/activate
export $(grep -v '^#' .env | xargs)
```
<a name="setup9"/>
### 9. Executar o DBT

Em seguida, execute:
```
cd dbt_project
dbt run
dbt test
```
Isso criar√° uma view no Big Query com as tabelas 
‚ÑπÔ∏è Todas as vari√°veis de ambiente (GCP, Prefect e DBT) s√£o centralizadas no arquivo .env para garantir seguran√ßa e facilidade de configura√ß√£o.




